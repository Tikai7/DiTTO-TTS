{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from model.Speech2Text import Speech2Text\n",
    "from model.SpeechGenerator import SpeechGenerator\n",
    "from utils.Config import ConfigSLP, ConfigNAC, ConfigDiTTO\n",
    "from utils.MLS import MLSDataset\n",
    "from utils.Processing import Processing\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigSLP.display()\n",
    "ConfigNAC.display()\n",
    "ConfigDiTTO.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TRAIN_PATH+\"/\"+\"audio\", ConfigSLP.TRAIN_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TEST_PATH+\"/\"+\"audio\", ConfigSLP.TEST_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.DEV_PATH+\"/\"+\"audio\", ConfigSLP.DEV_PATH+\"/\"+\"audio_clean\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Generation with DiTTO-TTs and Vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MLSDataset(\n",
    "    data_dir=ConfigDiTTO.TRAIN_PATH,\n",
    "    max_text_token_length=ConfigDiTTO.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigDiTTO.SAMPLE_RATE,\n",
    "    nb_samples=ConfigDiTTO.NB_SAMPLES,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "test_set = MLSDataset(\n",
    "    data_dir=ConfigDiTTO.TEST_PATH,\n",
    "    max_text_token_length=ConfigDiTTO.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigDiTTO.SAMPLE_RATE,\n",
    "    nb_samples=ConfigDiTTO.NB_SAMPLES,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigDiTTO.DIFFUSION_STEPS = 1000\n",
    "\n",
    "# remove SLP in the Speech generator if you want to load it on sorbonne's PPTI (disk quota exceeded)\n",
    "speech_generator = SpeechGenerator(\n",
    "    nac_model_path=\"/tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/src/params/NAC_epoch_20.pth\",\n",
    "    ditto_model_path=\"/tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/src/params/DiTTO_epoch_20.pth\",\n",
    "    slp_path=\"/tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/src/params/SLP_epoch_20.pth\",\n",
    "    lambda_factor=ConfigNAC.LAMBDA_FACTOR,\n",
    "    sample_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    device=ConfigDiTTO.DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_loader(loader, prompt=None):\n",
    "    ConfigDiTTO.DIFFUSION_STEPS = 1000\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    batch[\"audio\"] = batch[\"audio\"].to(ConfigDiTTO.DEVICE)\n",
    "    batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(ConfigDiTTO.DEVICE)\n",
    "    batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(ConfigDiTTO.DEVICE)\n",
    "\n",
    "    is_tokenized =  prompt is None\n",
    "\n",
    "    for audio_tensor, padding_mask_audio, text_input  in zip(batch[\"audio\"], batch[\"padding_mask_audio\"],  batch[\"text\"][\"input_ids\"]):\n",
    "        prompt = prompt if prompt is not None else text_input.unsqueeze(0)\n",
    "        generated_waveform = speech_generator.generate_speech_from_audio_tensor(\n",
    "            audio_tensor.to(ConfigDiTTO.DEVICE).unsqueeze(0), \n",
    "            padding_mask_audio.to(ConfigDiTTO.DEVICE).unsqueeze(0),\n",
    "            prompt,\n",
    "            is_tokenized=is_tokenized\n",
    "        )\n",
    "        output_path = \"output.wav\"\n",
    "        torchaudio.save(output_path, generated_waveform.cpu(), ConfigDiTTO.SAMPLE_RATE)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = test_with_loader(train_loader, \"Bonjour, comment çava tout le monde ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigDiTTO.DIFFUSION_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CER and WER Computation\n",
    "- Train : \n",
    "    - CER score: 0.9305486490966351\n",
    "    - WER score: 0.9981549815498155\n",
    "- Test :\n",
    "    - CER score: 0.9305370442963544\n",
    "    - WER score: 0.9973509933774835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "model = Speech2Text(sampling_rate=16000)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def cer_wer_on_loader(loader, max_batch=5):\n",
    "    with torch.no_grad():\n",
    "        for i,batch in tqdm(enumerate(loader)):\n",
    "\n",
    "            batch[\"audio\"] = batch[\"audio\"].to(ConfigDiTTO.DEVICE)\n",
    "            batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(ConfigDiTTO.DEVICE)\n",
    "            batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(ConfigDiTTO.DEVICE)\n",
    "\n",
    "            for audio_tensor, padding_mask_audio, text_input  in zip(batch[\"audio\"], batch[\"padding_mask_audio\"],  batch[\"text\"][\"input_ids\"]):\n",
    "                generated_waveform = speech_generator.generate_speech_from_audio_tensor(\n",
    "                    audio_tensor.to(ConfigDiTTO.DEVICE).unsqueeze(0), \n",
    "                    padding_mask_audio.to(ConfigDiTTO.DEVICE).unsqueeze(0),\n",
    "                    text_input.unsqueeze(0),\n",
    "                    is_tokenized=True\n",
    "                )\n",
    "                transcription = model(generated_waveform)\n",
    "                predictions.extend(transcription)\n",
    "                \n",
    "            ref_texts = tokenizer.batch_decode(batch[\"text\"][\"input_ids\"].to(ConfigDiTTO.DEVICE), skip_special_tokens=True)\n",
    "            references.extend(ref_texts)\n",
    "            if i > max_batch:\n",
    "                break\n",
    "\n",
    "    # Calcul des métriques\n",
    "    cer_score = cer_metric.compute(predictions=predictions, references=references)\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(\"CER score:\", cer_score)\n",
    "    print(\"WER score:\", wer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_wer_on_loader(train_loader)\n",
    "cer_wer_on_loader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM-o and SIM-r Computation\n",
    "- Train :\n",
    "    - SIM-o score: 0.27285963\n",
    "    - SIM-r score: 0.010607217\n",
    "- Test : \n",
    "    - SIM-o score: 0.18861449\n",
    "    - SIM-r score: 0.009921809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "spk_recog = SpeakerRecognition.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"tmp\"\n",
    ")\n",
    "\n",
    "def extract_embedding(audio_tensor):\n",
    "    emb = spk_recog.encode_batch(audio_tensor).squeeze().detach().cpu().numpy()\n",
    "    return emb\n",
    "\n",
    "def compute_similarity(audio_emb1, audio_emb2):\n",
    "    return 1 - cosine(audio_emb1, audio_emb2)\n",
    "\n",
    "def compute_sim_o_sim_r(loader, max_batch=5):\n",
    "    similarities_o = []\n",
    "    similarities_r = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(loader)):\n",
    "            batch[\"audio\"] = batch[\"audio\"].to(ConfigDiTTO.DEVICE)\n",
    "            batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(ConfigDiTTO.DEVICE)\n",
    "            batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(ConfigDiTTO.DEVICE)\n",
    "            \n",
    "            batch_embeddings = [] \n",
    "\n",
    "\n",
    "            for audio_tensor, padding_mask_audio, text_input  in zip(batch[\"audio\"], batch[\"padding_mask_audio\"],  batch[\"text\"][\"input_ids\"]):\n",
    "                generated_waveform = speech_generator.generate_speech_from_audio_tensor(\n",
    "                    audio_tensor.to(ConfigDiTTO.DEVICE).unsqueeze(0), \n",
    "                    padding_mask_audio.to(ConfigDiTTO.DEVICE).unsqueeze(0),\n",
    "                    text_input.unsqueeze(0),\n",
    "                    is_tokenized=True\n",
    "                )\n",
    "                    \n",
    "                emb_ref = extract_embedding(audio_tensor) \n",
    "                emb_gen = extract_embedding(generated_waveform) \n",
    "\n",
    "                batch_embeddings.append((emb_ref, emb_gen))  # Stock embedding for SIM-r\n",
    "\n",
    "                sim_o = compute_similarity(emb_ref, emb_gen)\n",
    "                similarities_o.append(sim_o)\n",
    "\n",
    "            # For SIM-r\n",
    "            for idx, (emb_ref, emb_gen) in enumerate(batch_embeddings):\n",
    "                other_embeddings = [emb[0] for j, emb in enumerate(batch_embeddings) if j != idx] \n",
    "                if other_embeddings:  \n",
    "                    other_similarities = [compute_similarity(emb_other, emb_gen) for emb_other in other_embeddings]\n",
    "                    sim_r = compute_similarity(emb_ref, emb_gen) - np.mean(other_similarities) \n",
    "                else:\n",
    "                    sim_r = compute_similarity(emb_ref, emb_gen)  \n",
    "\n",
    "                similarities_r.append(sim_r)\n",
    "            if i > max_batch:\n",
    "                break\n",
    "\n",
    "    sim_o_score = np.mean(similarities_o)\n",
    "    sim_r_score = np.mean(similarities_r)\n",
    "\n",
    "    print(\"SIM-o score:\", sim_o_score)\n",
    "    print(\"SIM-r score:\", sim_r_score)\n",
    "\n",
    "    return sim_o_score, sim_r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sim_o_sim_r(train_loader)\n",
    "compute_sim_o_sim_r(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
