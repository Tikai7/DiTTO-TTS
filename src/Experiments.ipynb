{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.SpeechLP import SLP\n",
    "from model.NeuralAudioCodec import NAC\n",
    "\n",
    "from utils.Config import ConfigSLP, ConfigNAC\n",
    "from utils.MLS import MLSDataset\n",
    "from utils.Trainer import Trainer\n",
    "from utils.Processing import Processing\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Base Settings: #############\n",
      "\n",
      "Audio Settings:\n",
      "  Sample Rate: 24000 Hz\n",
      "  Min Audio Duration: 10 seconds\n",
      "  Max Audio Duration: 20 seconds\n",
      "\n",
      "Training Settings:\n",
      "  Betas: [0.9, 0.999]\n",
      "  Device: cuda\n",
      "\n",
      "Data Settings:\n",
      "  Train Path: /tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/data/mls_french_opus/train\n",
      "  Test Path: /tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/data/mls_french_opus/test\n",
      "  Dev Path: /tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/data/mls_french_opus/dev\n",
      "\n",
      "############# SLP Settings: #############\n",
      "\n",
      "Model Settings:\n",
      "  Model Name: SLP\n",
      "  Embedding Dimension: 1472\n",
      "  Number of Layers: 1\n",
      "  Number of Attention Heads: 1\n",
      "  Number of Classes: 11\n",
      "  Number of Samples: 10000\n",
      "  Batch Size: 8\n",
      "  Learning Rate: 0.0001\n",
      "  Epochs: 20\n",
      "  Token length for ByT5: 128\n"
     ]
    }
   ],
   "source": [
    "ConfigSLP.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TRAIN_PATH+\"/\"+\"audio\", ConfigSLP.TRAIN_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TEST_PATH+\"/\"+\"audio\", ConfigSLP.TEST_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.DEV_PATH+\"/\"+\"audio\", ConfigSLP.DEV_PATH+\"/\"+\"audio_clean\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.TRAIN_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    "    nb_samples = ConfigSLP.NB_SAMPLES\n",
    ")\n",
    "\n",
    "val_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.DEV_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    ")\n",
    "\n",
    "\n",
    "test_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.TEST_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slp = SLP(ConfigSLP.NB_CLASSES, ConfigSLP.NHEAD ,ConfigSLP.NUM_LAYERS)\n",
    "model_slp = model_slp.to(ConfigSLP.DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.set_model(model_slp, name=ConfigSLP.MODEL_NAME)\\\n",
    "    .set_criterion(criterion)\\\n",
    "    .set_optimizer(optimizer)\\\n",
    "    .fit(\n",
    "        train_data=train_loader, validation_data=val_loader, \n",
    "        epochs=ConfigSLP.EPOCHS, learning_rate=ConfigSLP.LEARNING_RATE, checkpoint_interval=1        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.TRAIN_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    nb_samples = ConfigNAC.NB_SAMPLES,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "val_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.DEV_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "test_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.TEST_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tempory/M2-DAC/UE_DEEP/AMAL/DiTTO-TTS/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "model_nac = NAC(ConfigNAC.LAMBDA_FACTOR)\n",
    "model_nac = model_nac.to(ConfigNAC.DEVICE)\n",
    "optimizer = torch.optim.AdamW\n",
    "\n",
    "\n",
    "def train(self, train_loader):\n",
    "    losses = 0\n",
    "    self.model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(self.device)\n",
    "        batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(self.device)\n",
    "\n",
    "        text = batch[\"text\"]\n",
    "        audio = batch[\"audio\"].to(self.device)\n",
    "        padding_mask_audio = batch[\"padding_mask_audio\"].to(self.device)\n",
    "\n",
    "        output = self.model(text, audio, padding_mask_audio)\n",
    "        loss = output[\"total_loss\"]\n",
    "\n",
    "        losses += loss.item()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    return losses / len(train_loader), {\"lm_loss\" : output[\"lm_loss\"], \"reconstruction_loss\": output[\"reconstruction_loss\"]}\n",
    "\n",
    "\n",
    "def validation(self, validation_loader):\n",
    "    losses = 0\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_loader):\n",
    "            batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(self.device)\n",
    "            batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(self.device)\n",
    "            \n",
    "            text = batch[\"text\"]\n",
    "            audio = batch[\"audio\"].to(self.device)\n",
    "            padding_mask_audio = batch[\"padding_mask_audio\"].to(self.device)\n",
    "\n",
    "            output = self.model(text, audio, padding_mask_audio)\n",
    "            loss = output[\"total_loss\"]\n",
    "            \n",
    "            losses += loss.item()\n",
    "\n",
    "    return losses / len(validation_loader), {\"lm_loss\" : output[\"lm_loss\"], \"reconstruction_loss\": output[\"reconstruction_loss\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.set_model(model_nac, name=ConfigNAC.MODEL_NAME)\\\n",
    "    .set_criterion(torch.nn.MSELoss)\\\n",
    "    .set_optimizer(optimizer)\\\n",
    "    .set_custom_functions(train_func=train, validation_func=validation)\\\n",
    "    .fit(\n",
    "        train_data=train_loader, validation_data=val_loader, \n",
    "        epochs=ConfigNAC.EPOCHS, learning_rate=ConfigNAC.LEARNING_RATE, checkpoint_interval=1        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
