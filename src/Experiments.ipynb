{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.SpeechLP import SLP\n",
    "from model.NeuralAudioCodec import NAC\n",
    "\n",
    "from utils.Config import ConfigSLP, ConfigNAC\n",
    "from utils.MLS import MLSDataset\n",
    "from utils.Trainer import Trainer\n",
    "from utils.Processing import Processing\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigSLP.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TRAIN_PATH+\"/\"+\"audio\", ConfigSLP.TRAIN_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.TEST_PATH+\"/\"+\"audio\", ConfigSLP.TEST_PATH+\"/\"+\"audio_clean\",)\n",
    "# Processing.remove_metadata_from_audio_folder(ConfigSLP.DEV_PATH+\"/\"+\"audio\", ConfigSLP.DEV_PATH+\"/\"+\"audio_clean\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.TRAIN_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    "    nb_samples = ConfigSLP.NB_SAMPLES\n",
    ")\n",
    "\n",
    "val_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.DEV_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    ")\n",
    "\n",
    "\n",
    "test_set = MLSDataset(\n",
    "    data_dir=ConfigSLP.TEST_PATH,\n",
    "    max_text_token_length=ConfigSLP.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigSLP.SAMPLE_RATE,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=ConfigSLP.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slp = SLP(ConfigSLP.NB_CLASSES, ConfigSLP.NHEAD ,ConfigSLP.NUM_LAYERS)\n",
    "model_slp = model_slp.to(ConfigSLP.DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.set_model(model_slp, name=ConfigSLP.MODEL_NAME)\\\n",
    "    .set_criterion(criterion)\\\n",
    "    .set_optimizer(optimizer)\\\n",
    "    .fit(\n",
    "        train_data=train_loader, validation_data=val_loader, \n",
    "        epochs=ConfigSLP.EPOCHS, learning_rate=ConfigSLP.LEARNING_RATE, checkpoint_interval=1        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.TRAIN_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    nb_samples = ConfigNAC.NB_SAMPLES,\n",
    "    tokenizer_model=\"gpt2\"\n",
    ")\n",
    "\n",
    "val_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.DEV_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    tokenizer_model=\"gpt2\"\n",
    "\n",
    ")\n",
    "\n",
    "test_set = MLSDataset(\n",
    "    data_dir=ConfigNAC.TEST_PATH,\n",
    "    max_text_token_length=ConfigNAC.MAX_TOKEN_LENGTH,\n",
    "    sampling_rate=ConfigNAC.SAMPLE_RATE,\n",
    "    tokenizer_model=\"gpt2\"\n",
    "\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=ConfigNAC.BATCH_SIZE, shuffle=True, collate_fn=MLSDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nac = NAC(ConfigNAC.LAMBDA_FACTOR)\n",
    "model_nac = model_nac.to(ConfigNAC.DEVICE)\n",
    "optimizer = torch.optim.AdamW\n",
    "\n",
    "\n",
    "def train(self, train_loader):\n",
    "    losses = 0\n",
    "    self.model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(self.device)\n",
    "        batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(self.device)\n",
    "\n",
    "        text = batch[\"text\"]\n",
    "        audio = batch[\"audio\"].to(self.device)\n",
    "        padding_mask_audio = batch[\"padding_mask_audio\"].to(self.device)\n",
    "\n",
    "        output = self.model(text, audio, padding_mask_audio)\n",
    "        loss = output[\"total_loss\"]\n",
    "\n",
    "        losses += loss.item()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    return losses / len(train_loader), {\"lm_loss\" : output[\"lm_loss\"], \"reconstruction_loss\": output[\"reconstruction_loss\"]}\n",
    "\n",
    "def validation(self, validation_loader):\n",
    "    losses = 0\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_loader):\n",
    "            batch[\"text\"][\"input_ids\"] = batch[\"text\"][\"input_ids\"].to(self.device)\n",
    "            batch[\"text\"][\"attention_mask\"] = batch[\"text\"][\"attention_mask\"].to(self.device)\n",
    "            \n",
    "            text = batch[\"text\"]\n",
    "            audio = batch[\"audio\"].to(self.device)\n",
    "            padding_mask_audio = batch[\"padding_mask_audio\"].to(self.device)\n",
    "\n",
    "            output = self.model(text, audio, padding_mask_audio)\n",
    "            loss = output[\"total_loss\"]\n",
    "            \n",
    "            losses += loss.item()\n",
    "\n",
    "    return losses / len(validation_loader), {\"lm_loss\" : output[\"lm_loss\"], \"reconstruction_loss\": output[\"reconstruction_loss\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.set_model(model_nac, name=ConfigNAC.MODEL_NAME)\\\n",
    "    .set_criterion(torch.nn.MSELoss)\\\n",
    "    .set_optimizer(optimizer)\\\n",
    "    .set_custom_functions(train_func=train, validation_func=validation)\\\n",
    "    .fit(\n",
    "        train_data=train_loader, validation_data=val_loader, \n",
    "        epochs=ConfigNAC.EPOCHS, learning_rate=ConfigNAC.LEARNING_RATE, checkpoint_interval=1        \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
